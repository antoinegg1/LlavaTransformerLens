{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/LLaMA.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chameleon in TransformerLens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_592229/659676451.py:20: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_592229/659676451.py:21: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using renderer: colab\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "import os\n",
    "\n",
    "DEVELOPMENT_MODE = False\n",
    "IN_VSCODE = False\n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")\n",
    "    \n",
    "# %pip install transformers>=4.31.0 # Llama requires transformers>=4.31.0 and transformers in turn requires Python 3.8\n",
    "# %pip install sentencepiece # Llama tokenizer requires sentencepiece\n",
    "\n",
    "if IN_COLAB or IN_GITHUB:\n",
    "    %pip install torch\n",
    "    %pip install transformer_lens\n",
    "    %pip install circuitsvis\n",
    "    \n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")\n",
    "\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "# from transformers import ChameleonModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Float\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/aifs4su/yaodong/changye/TransformerLens')\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.HookedLlava import HookedLlava\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Chameleon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to load local chameleon model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47485af93f7f4bf4a556697f125b41f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODEL_PATH = \"/aifs4su/yaodong/projects/hantao/dev_cham/align-anything/outputs/0830_4k_sft_flux\"\n",
    "MODEL_PATH = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "# MODEL_PATH = \"/aifs4su/yaodong/models/chameleon-7b-hf\"\n",
    "# MODEL_PATH=\"/aifs4su/yaodong/projects/hantao/anole/facilitating_image_generation/model/chameleon_hf_0830_4k\"\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(MODEL_PATH)\n",
    "vision_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH, \n",
    "        torch_dtype=torch.float32, \n",
    "        low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "hf_model=vision_model.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llava-hf/llava-v1.6-mistral-7b-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# 将 vision_tower 和 multi_modal_projector 分配到 cuda:0\n",
    "    vision_tower = vision_model.vision_tower.to(\"cuda:0\")\n",
    "    multi_modal_projector = vision_model.multi_modal_projector.to(\"cuda:0\")\n",
    "    \n",
    "    # HookedTransformer 语言模型分配到 cuda:1\n",
    "    hook_language_model = HookedLlava.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        hf_model=vision_model.language_model,\n",
    "        vision_tower=vision_tower,\n",
    "        multi_modal_projector=multi_modal_projector,\n",
    "        device=\"cuda:1\",  # 放在cuda:1\n",
    "        fold_ln=False,\n",
    "        center_writing_weights=False,\n",
    "        center_unembed=False,\n",
    "        tokenizer=None,\n",
    "        dtype=torch.float32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_and_idxs = list(zip(range(model.cfg.n_layers), model.blocks))\n",
    "for i, block in blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_blocks_and_idxs = list(zip(range(hf_model.config.num_hidden_layers), hf_model.model.layers))\n",
    "\n",
    "for i, block in hf_blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")\n",
    "# print(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_params = model.state_dict()\n",
    "hf_block_params = hf_model.state_dict()\n",
    "print(block_params.keys())\n",
    "print(hf_block_params.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (i, block), (j, hf_block) in zip(blocks_and_idxs, hf_blocks_and_idxs):\n",
    "#     assert block == hf_block, f\"Block {i} does not match: {block} vs {hf_block}\"\n",
    "# print(model.blocks[0].attn.query.weight.shape)\n",
    "import einops\n",
    "for i in range(model.cfg.n_layers):\n",
    "    W_Q=einops.rearrange(block_params[f\"blocks.{i}.attn.W_Q\"], \"n m h -> (n h) m\")\n",
    "    W_K=einops.rearrange(block_params[f\"blocks.{i}.attn.W_K\"], \"n m h -> (n h) m\")\n",
    "    W_V=einops.rearrange(block_params[f\"blocks.{i}.attn.W_V\"], \"n m h -> (n h) m\")\n",
    "    W_O=einops.rearrange(block_params[f\"blocks.{i}.attn.W_O\"], \"n h m -> m (n h)\")\n",
    "    \n",
    "    device = \"cuda:2\"\n",
    "    if not torch.equal(W_Q.to(device),hf_block_params[f\"model.layers.{i}.self_attn.q_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_Q does not match\")\n",
    "    if not torch.equal(W_K.to(device),hf_block_params[f\"model.layers.{i}.self_attn.k_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_K does not match\")\n",
    "    if not torch.equal(W_V.to(device),hf_block_params[f\"model.layers.{i}.self_attn.v_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_V does not match\")\n",
    "    if not torch.equal(W_O.to(device),hf_block_params[f\"model.layers.{i}.self_attn.o_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_O does not match\")\n",
    "    # print(torch.equal(W_Q.to(device),hf_block_params[f\"model.layers.{i}.self_attn.q_proj.weight\"].to(device)))\n",
    "# W_Q=einops.rearrange(block_params[f\"blocks.{0}.attn.W_Q\"], \"n m h -> (n h) m\")\n",
    "# print(W_Q.shape)\n",
    "# device = \"cuda:2\"\n",
    "# print(torch.equal(W_Q.to(device),hf_block_params[f\"model.layers.{0}.self_attn.q_proj.weight\"].to(device)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.blocks[0].attn.norm_Q.weight)\n",
    "# print(\"=\"*10)\n",
    "# print(model.blocks[0].attn.norm_Q.bias)\n",
    "# print(\"=\"*10)\n",
    "# print(model.blocks[0].attn.norm_K.weight)\n",
    "# print(\"=\"*10)\n",
    "# print(model.blocks[0].attn.norm_K.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Where is the capital of Germany?\"\n",
    "input = processor(prompt, return_tensors=\"pt\")\n",
    "input_ids = input.input_ids\n",
    "print(input_ids)\n",
    "output = model.generate(input_ids)\n",
    "print(processor.tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "        \"The capital of Germany is\",\n",
    "        \"2 * 42 = \", \n",
    "        \"My favorite\", \n",
    "        \"aosetuhaosuh aostud aoestuaoentsudhasuh aos tasat naostutshaosuhtnaoe usaho uaotsnhuaosntuhaosntu haouaoshat u saotheu saonuh aoesntuhaosut aosu thaosu thaoustaho usaothusaothuao sutao sutaotduaoetudet uaosthuao uaostuaoeu aostouhsaonh aosnthuaoscnuhaoshkbaoesnit haosuhaoe uasotehusntaosn.p.uo ksoentudhao ustahoeuaso usant.hsa otuhaotsi aostuhs\",\n",
    "    ]\n",
    "    \n",
    "    # 切换到评估模式\n",
    "model.eval()\n",
    "hf_model.eval()\n",
    "tokenizer=AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "# print(tokenizer)\n",
    "# 将模型参数移动到 GPU 上\n",
    "model_device =\"cuda:0\"\n",
    "hf_model_device = \"cuda:1\"\n",
    "model=model.to(model_device)\n",
    "hf_model=hf_model.to(hf_model_device)\n",
    "    \n",
    "    # 分别处理每一个 prompt，避免一次性加载太多\n",
    "# 分别处理每一个 prompt，避免一次性加载太多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 定义 prompt 和 tokenizer\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# 假设我们使用 Hugging Face 模型的 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 分别对 HookedTransformer 和 Hugging Face 模型进行编码\n",
    "prompt_id_tl = tokenizer.encode(prompt, return_tensors=\"pt\").to(model_device)  # HookedTransformer 模型输入\n",
    "prompt_id_hf = tokenizer.encode(prompt, return_tensors=\"pt\").to(hf_model_device)  # Hugging Face 模型输入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: 定义钩子函数和输出字典\n",
    "\n",
    "# 定义一个钩子函数来捕获子模块的输出\n",
    "def hook_fn(module_name, module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]  # 如果输出是元组，取第一个输出\n",
    "    return {module_name: output.detach().cpu()}\n",
    "\n",
    "# 创建字典来存储 HookedTransformer 和 Hugging Face 模型的输出\n",
    "tl_internal_outputs = {}\n",
    "hf_internal_outputs = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: 注册钩子\n",
    "\n",
    "# 为 Hugging Face 模型的主要子模块（input_layernorm, self_attn, mlp）添加钩子\n",
    "def register_hf_hooks(hf_model):\n",
    "    hf_model.model.layers[0].input_layernorm.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"input_layernorm\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.q_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.q_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.o_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.o_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.gate_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.gate_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.down_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.down_proj\", m, i, o)))\n",
    "\n",
    "# 为 HookedTransformer 模型的各个子模块添加钩子\n",
    "def register_tl_hooks(model):\n",
    "    model.blocks[0].hook_resid_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_resid_pre\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_q.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_in\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_z.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_out\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_in\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_post.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_out\", m, i, o)))\n",
    "\n",
    "# 注册 Hugging Face 模型钩子\n",
    "register_hf_hooks(hf_model)\n",
    "\n",
    "# 注册 HookedTransformer 模型钩子\n",
    "register_tl_hooks(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf_model.device)\n",
    "print(prompt_id_hf.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: 运行模型前向传播\n",
    "tl_logits = model(prompt_id_tl).detach().cpu()\n",
    "hf_logits = hf_model(prompt_id_hf).logits.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: 比较两个模型的中间输出\n",
    "module_mapping = {\n",
    "    \"hook_attn_in\": \"self_attn.q_proj\",\n",
    "    \"hook_attn_out\": \"self_attn.o_proj\",\n",
    "    \"hook_mlp_in\": \"mlp.gate_proj\",\n",
    "    \"hook_mlp_out\": \"mlp.down_proj\"\n",
    "}\n",
    "\n",
    "for tl_key, hf_key in module_mapping.items():\n",
    "    if tl_key in tl_internal_outputs and hf_key in hf_internal_outputs:\n",
    "        tl_value = tl_internal_outputs[tl_key]\n",
    "        hf_value = hf_internal_outputs[hf_key]\n",
    "        print(tl_value.shape)\n",
    "        \n",
    "        print(hf_value.shape)\n",
    "        if tl_key==\"hook_attn_in\" or tl_key==\"hook_attn_out\":\n",
    "            tl_value=tl_value.reshape(1,8,4096)\n",
    "\n",
    "        if not torch.allclose(tl_value, hf_value, atol=1e-4, rtol=1e-2):\n",
    "            print(f\"Difference found in {tl_key} (TL) vs {hf_key} (HF):\")\n",
    "            print(f\"HookedTransformer output: {tl_value}\")\n",
    "            print(f\"Hugging Face output: {hf_value}\")\n",
    "            print(f\"Difference: {tl_value - hf_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个钩子函数来捕获子模块的输出\n",
    "def hook_fn(module_name, module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]  # 如果输出是元组，取第一个输出\n",
    "    return {module_name: output.detach().cpu()}\n",
    "\n",
    "# 创建字典来存储 HookedTransformer 和 Hugging Face 模型的输出\n",
    "tl_internal_outputs = {}\n",
    "hf_internal_outputs = {}\n",
    "\n",
    "# 映射表，定义 HookedTransformer 模型和 Hugging Face 模型的模块映射\n",
    "module_mapping = {\n",
    "    \"hook_resid_pre\": \"input_layernorm\",\n",
    "    \"hook_attn_in\": \"self_attn.q_proj\",\n",
    "    \"hook_attn_out\": \"self_attn.o_proj\",\n",
    "    \"hook_mlp_in\": \"mlp.gate_proj\",\n",
    "    \"hook_mlp_out\": \"mlp.down_proj\"\n",
    "}\n",
    "\n",
    "# 为 Hugging Face 模型的主要子模块（input_layernorm, self_attn, mlp）添加钩子\n",
    "def register_hf_hooks(hf_model):\n",
    "    hf_model.model.layers[0].input_layernorm.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"input_layernorm\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.q_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.q_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.o_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.o_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.gate_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.gate_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.down_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.down_proj\", m, i, o)))\n",
    "\n",
    "# 为 HookedTransformer 模型的各个子模块添加钩子\n",
    "def register_tl_hooks(model):\n",
    "    model.blocks[0].hook_resid_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_resid_pre\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_q.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_in\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_z.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_out\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_in\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_post.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_out\", m, i, o)))\n",
    "\n",
    "# 注册 Hugging Face 模型钩子\n",
    "register_hf_hooks(hf_model)\n",
    "\n",
    "# 注册 HookedTransformer 模型钩子\n",
    "register_tl_hooks(model)\n",
    "# 定义你的 prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# 使用 tokenizer 对 prompt 进行编码\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 对 HookedTransformer 模型和 Hugging Face 模型分别进行编码\n",
    "prompt_id_tl = tokenizer.encode(prompt, return_tensors=\"pt\").to(model_device)  # HookedTransformer 模型的输入\n",
    "prompt_id_hf = tokenizer.encode(prompt, return_tensors=\"pt\").to(hf_model_device)  # Hugging Face 模型的输入\n",
    "# 运行模型前向传播\n",
    "tl_logits = model(prompt_id_tl).detach().cpu()\n",
    "hf_logits = hf_model(prompt_id_hf).logits.detach().cpu()\n",
    "\n",
    "# 比较两个模型的中间输出\n",
    "for tl_key, hf_key in module_mapping.items():\n",
    "    if tl_key in tl_internal_outputs and hf_key in hf_internal_outputs:\n",
    "        tl_value = tl_internal_outputs[tl_key]\n",
    "        hf_value = hf_internal_outputs[hf_key]\n",
    "        if not torch.allclose(tl_value, hf_value, atol=1e-4, rtol=1e-2):\n",
    "            print(f\"Difference found in {tl_key} (TL) vs {hf_key} (HF):\")\n",
    "            print(f\"HookedTransformer output: {tl_value}\")\n",
    "            print(f\"Hugging Face output: {hf_value}\")\n",
    "            print(f\"Difference: {tl_value - hf_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 分别处理每一个 prompt，避免一次性加载太多\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Processing prompt {i+1}/{len(prompts)}\")\n",
    "\n",
    "    # 将输入移动到模型所在的设备\n",
    "    prompt_id = tokenizer.encode(prompt, return_tensors=\"pt\").to(model_device)\n",
    "    prompt_id_hf = tokenizer.encode(prompt, return_tensors=\"pt\").to(hf_model_device)\n",
    "\n",
    "    # 获取第 0 层的输入\n",
    "    tl_input = \n",
    "    hf_input = prompt_id_hf\n",
    "    \n",
    "    # 执行第 0 层的前向传播\n",
    "    tl_layer_output = model.blocks[0](tl_input)\n",
    "    hf_layer_output = hf_model.model.layers[0](hf_input)\n",
    "\n",
    "    # 比较第 0 层的输出\n",
    "    if not torch.allclose(hf_layer_output, tl_layer_output, atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Difference found at layer 0 for prompt {i}:\")\n",
    "        print(f\"hf_layer_output: {hf_layer_output}\")\n",
    "        print(f\"tl_layer_output: {tl_layer_output}\")\n",
    "        print(f\"Difference: {hf_layer_output - tl_layer_output}\")\n",
    "        \n",
    "        # 打印最大绝对误差和相对误差\n",
    "        abs_diff = torch.max(torch.abs(hf_layer_output - tl_layer_output))\n",
    "        rel_diff = torch.max(torch.abs((hf_layer_output - tl_layer_output) / (tl_layer_output + 1e-8)))\n",
    "        print(f\"Max absolute difference at layer 0: {abs_diff.item()}\")\n",
    "        print(f\"Max relative difference at layer 0: {rel_diff.item()}\")\n",
    "\n",
    "        # 放宽误差条件后再检查\n",
    "        if not torch.allclose(hf_layer_output, tl_layer_output, atol=1e-3, rtol=1e-2):\n",
    "            print(f\"Larger difference persists at layer 0 for prompt {i}, investigate further.\")\n",
    "\n",
    "    # 断言条件，严格验证差异\n",
    "    assert torch.allclose(hf_layer_output, tl_layer_output, atol=1e-4, rtol=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def process_image_and_generate_response(processor, vision_model, image_path):\n",
    "    \"\"\"\n",
    "    加载图像并生成图像描述。\n",
    "    \"\"\"\n",
    "    # 加载本地图像\n",
    "    image = Image.open(image_path)\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    \n",
    "    # 处理图像和文本输入\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "    \n",
    "    return inputs\n",
    "image_path = \"/aifs4su/yaodong/changye/TransformerLens/IMG_20230213_181559.jpg\"\n",
    "inputs = process_image_and_generate_response(processor, vision_model, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs_embeds, input_ids, position_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs_embeds)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids)\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/aifs4su/yaodong/changye/TransformerLens/transformer_lens/HookedLlava.py:2990\u001b[0m, in \u001b[0;36mHookedLlava.get_embedding\u001b[0;34m(self, inputs, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, use_past_kv_cache, prepend_bos, padding_side, return_type, verbose)\u001b[0m\n\u001b[1;32m   2986\u001b[0m     \u001b[38;5;66;03m# pdb.set_trace()\u001b[39;00m\n\u001b[1;32m   2987\u001b[0m     \u001b[38;5;66;03m# Currently nothing in HookedTransformer changes with eval, but this is here in case\u001b[39;00m\n\u001b[1;32m   2988\u001b[0m     \u001b[38;5;66;03m# that changes in the future.\u001b[39;00m\n\u001b[1;32m   2989\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m-> 2990\u001b[0m     inputs_embeds, input_ids, position_ids, attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVL_to_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m                \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# past_kv_cache=past_kv_cache,\u001b[39;49;00m\n\u001b[1;32m   2995\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m                \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs_embeds, input_ids, position_ids, attention_mask\n",
      "File \u001b[0;32m/aifs4su/yaodong/changye/TransformerLens/transformer_lens/HookedLlava.py:918\u001b[0m, in \u001b[0;36mHookedLlava.VL_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, attention_mask, pixel_values, image_sizes)\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixel_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expect to be of 4 or 5 dimensions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    917\u001b[0m device\u001b[38;5;241m=\u001b[39mpixel_values\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_tower\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m \u001b[38;5;241m!=\u001b[39m device:\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_tower\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_modal_projector\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "inputs_embeds, input_ids, position_ids, attention_mask=model.get_embedding(inputs)\n",
    "print(inputs_embeds)\n",
    "print(input_ids)\n",
    "print(position_ids)\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"<image> Tell me what is in the image?\"\n",
    "# image_path = \"/aifs4su/yaodong/datasets/aaa_dataset/T2I-preference/0812_t2i_preference_dataset/image/285ed1502f68ad9737af9b4c059d76b82984421692a22f08eff793a0cc6301e3/36ad46bb0f79b5eedaed2818b2744a19d5f89e52e410b43140a875daabebf088.png\"\n",
    "# torch.set_printoptions(threshold=torch.inf)\n",
    "# from PIL import Image\n",
    "\n",
    "# image = Image.open(image_path)\n",
    "# input = processor(prompt, image, return_tensors=\"pt\").to(\n",
    "#             hf_model.device, dtype=hf_model.dtype\n",
    "#         )\n",
    "# input_ids = input.input_ids\n",
    "# print(input_ids)\n",
    "input_ids = torch.tensor([[    0,  8197, \n",
    "                          3643,   285,  5867,   453,  6540,  6488,   376,  7347,  6468,  1656,\n",
    "         1057,  1910,  6482,  4008,  5611,   376,  3706,  2941,  7444,  7444,\n",
    "          712,  5664,  4476,  3234,  6307,   798,  7444,  6345,  5589,  3158,\n",
    "           40,  1323,   376,  3643,  3181,  1641,   448,  3743,  2895,  1971,\n",
    "          376,  3642,  5439,  6097,  6820,   226,  3666,   750,  5867,   269,\n",
    "         7444,  1343,  4214,  2229,  1093,  3000,  6009,  3214,  7755,  4599,\n",
    "         6420,  5409,  7873,  3396,  6614,   827,  3872,  1741,  7536,   581,\n",
    "         7349,  4187,  7444,  7444,  3706,   376,  4317,  1303,  7518,  1062,\n",
    "         1846,  3214,  6488,  4746,  2419,  4462,  6035,  6498,  5104,  7634,\n",
    "         4129,  2991,  3356,   182,  2872,  4187,  7903,  4237,  2095,  2873,\n",
    "         4359,  2128,  4405,  1527,  2895,  3772,   798,  1385,  3643,   362,\n",
    "         7065,  6305,  4102,   376,  5906,  1910,   805,  4502,  2868,  3048,\n",
    "         8015,  2217,  1596,  2029,  7303,  6045,  7626,  1930,  4472,   251,\n",
    "         4591,  6811,  7162,  6684,  3048,  2359,  2394,  3214,  4134,  2941,\n",
    "         7783,  1846,   610,  2953,   863,   805,  6306,    37,  6113,  1238,\n",
    "         6486,  7991,  7729,  1689,  2290,   651,  6891,  5312,  2644,  5291,\n",
    "         4384,  4274,  3195,  3296,  6435,  4983,   589,  1846,   481,    10,\n",
    "         4707,  3320,  6529,   587,  1578,  2659,  6248,  6482,  3214,  4503,\n",
    "         2881,  5021,   350,  5054,  4234,   685,  6805,  1424,    41,  5274,\n",
    "         3911,   453,  4084,  5711,    56,  7528,  7720,  5813,  5918,  2348,\n",
    "          792,  1787,  3025,  6037,  3706,  6820,   805,  3412,  7444,  7893,\n",
    "         6291,  2547,  1846,  3214,  1324,  6078,  5267,   249,  1240,  6435,\n",
    "         6761,  5366,  4488,   777,   238,  4554,  4436,  1541,  4187,  1930,\n",
    "         5949,  3983,  1028,  4661,  5054,  4503,  3911,  4659,  3144,   481,\n",
    "          792,  3144,  5592,   249,  7395,    77,   363,  4309,  1067,  1590,\n",
    "         2202,  3643,  2001,  2982,  7201,   604,  4666,  2325,  3110,    63,\n",
    "         4400,    10,  5730,  6778,   219,  4149,  7905,  5432,  2658,  4642,\n",
    "         8044,  4790,  4440,   312,  1414,   625,  2182,   363,  2225,  4649,\n",
    "         6874,  3163,  4791,  5441,  7229,   362,  2466,   191,   481,  4498,\n",
    "         6922,  1324,  3605,  1272,   863,  2805,  2561,  7189,  4593,  5192,\n",
    "         3859,  4180,  3218,  3188,  4149,  3252,  3437,  2317,  2820,    10,\n",
    "         7749,  7569,   675,  1847,  3772,  3427,  3584,  7437,  5447,  3083,\n",
    "         3248,   604,  4129,  5488,  1245,  2001,  6906,  5936,  6718,  1351,\n",
    "         1022,  6750,  3609,  3549,   177,  1526,  7893,  7380,  1306,  2557,\n",
    "         1237,  1218,  1106,  4276,  3610,  6358,  2132,  4440,  5137,   742,\n",
    "         4869,  5296,   363,  5137,  3204,  7036,  4077,  3819,  2269,  3197,\n",
    "         6512,  1748,  1292,  4077,  4681,  1091,  7647,  3819,  7489,  2886,\n",
    "         6380,  4659,  2001,  4284,  5680,  3020,  6622,   930,  5124,  4489,\n",
    "         1876,  3000,  3819,  2454,  3248,   363,   363,  5519,  2992,  6775,\n",
    "         7334,  2132,  6068,  1022,  3221,  4077,  2347,   363,  1754,  4422,\n",
    "         7647,  2886,  2132,  3452,  6725,  4676,  7102,  1028,  4086,  2066,\n",
    "         4128,  7031,  2501,  4642,  6291,  5866,  1858,  3963,  2132,  3437,\n",
    "          302,  4818,   445,  1106,  4529,  1681,  1641,  5540,  2001,   363,\n",
    "         7489,  3605,  7647,  2132,  4354,  1930,  7402,  5192,   953,  5946,\n",
    "         7524,  3810,  6072,  3181,  3020,  5486,  3382,  2672,   407,  3810,\n",
    "         4671,  6522,  6782,  7547,  6115,  2132,  5716,  5375,  7450,  3661,\n",
    "         4211,  1106,  2590,  1861,  3601,  3356,  1263,  3540,  1208,  6360,\n",
    "         3900,  5278,   862,  6607,   103,  4537,  4446,  3206,  5239,  2908,\n",
    "         6584,  1858,  2284,  3319,  5503,  4135,  4671,  5307,  7631,   354,\n",
    "         5293,  4374,   221,  2749,  6861,  4211,  3665,  4029,  5307,  1682,\n",
    "         3188,  1245,    19,   502,   390,  5798,  4239,  3020,  4686,  4184,\n",
    "         4184,  2243,  3831,  3831,  5706,  6260,  2240,  3135,  4259,  2064,\n",
    "         3168,  8081,  3020,  1974,  1016,  4180,  4146,  6085,   637,  1331,\n",
    "         1079,   741,  6725,  3862,  3190,  1745,  5779,  1453,  5402,  4821,\n",
    "         2036,  5866,  1578,  1298,  1706,  3631,   390,  5861,  1708,  1938,\n",
    "         5034,  1049,  6524,  1876,  1648,  4574,  5074,  8044,  4317,  1882,\n",
    "          975,  2900,  4251,  2900,  1420,   133,  1453,  1077,  2784,  1332,\n",
    "         4837,  2773,  3244,  7514,  4416,   894,   346,  5483,  6345,  7622,\n",
    "         5486,  1642,  2787,  5378,   973,  7099,   614,  7748,  4953,   777,\n",
    "         5127,  5757,  1558,  4729,  7292,  1331,  3420,  2401,  2731,  1298,\n",
    "          943,  6629,  5838,  5905,  7935,  4163,  6199,   551,  4968,  4951,\n",
    "          141,  3181,  3155,   480,  4512,  3544,  2318,  2145,  4213,  6852,\n",
    "         8147,  4300,  3446,  6511,  3891,  7374,  7774,  5906,  1659,  5592,\n",
    "         3096,  4192,   229,  3242,  4477,  3688,  4298,  5618,  2445,  1628,\n",
    "         1557,  3841,   833,   381,  7065,  3762,  3172,  3020,   689,  4146,\n",
    "         1838,  7729,  7287,   768,  2291,  3008,  3543,  7864,  6028,   959,\n",
    "         7603,  2508,   142,  7528,  4254,  6896,  7075,  2941,  1264,  1582,\n",
    "         1930,  2048,   543,  6139,  2961,  4405,  2289,  2004,  5865,  8030,\n",
    "         6524,  3008,  6438,  5278,  4266,  2291,  4407,  1181,  3296,  3110,\n",
    "         5652,  5320,  6390,  2659,  1018,  4192,  7592,  5723,  5564,  1237,\n",
    "         2268,  3296,  4029,  1057,   520,  2463,  4883,    79,  2514,  6390,\n",
    "          693,  1611,  3610,  3946,  1832,  6800,  4478,   177,  7990,  2941,\n",
    "         7154,  2895,  4877,  3606,  1522,   854,  3172,  6290,  3559,   905,\n",
    "         4405,  1630,  7347,  3778,  4029,  4484,  5453,  6191,  7183,  3769,\n",
    "         7510,  4686,  6629,  4615,  3042,  5681,  1440,   630,  7011,  5342,\n",
    "         2380,  3645,   467,  3155,  5774,  1166,  1459,  7347,  5103,   249,\n",
    "          346,  5664,  6291,  8044,  8067,  4986,  4462,  2245,  5578,  1215,\n",
    "         3108,  4331,  5271,  1645,  1202,  1288,  5554,  1745,  1331,  7032,\n",
    "         1028,  1067,  6462,  1630,   947,  6155,  3016,  5278,   242,  5785,\n",
    "         2869,  2744,  2744,  7626,  8187,  1503,  3636,  2716,  4485,  7047,\n",
    "         2443,  2372,  3008,  1846,  1363,  2036,  3108,   312,  5428,  4976,\n",
    "           46,  2466,  3255,  1930,  5892,  4503,  5320,  2113,  1584,  2338,\n",
    "          318,   476,  2998,  3907,  1747,   742,  2492,  3808,  5525,   386,\n",
    "         5362,  4849,   930,  3767,  1213,  1067,  3103,  5026,  8160,  4821,\n",
    "         2126,  5313,  4239,  2287,  3252,  3214,  1414,  1526,  2645,  6111,\n",
    "         1317,  5478,   382,  4780,  2012,  6664,  5682,  3769,  1832,  4530,\n",
    "         4512,   275,   714,   474,  2021,  6706,  2291,   355,  4276,  5376,\n",
    "         7195,  5901,   610,  2268,  2941,  1223,  4574,  2287,  4008,  1810,\n",
    "         1420,  7078,  2765,   635,  3867,  5662,    79,  8140,  7820,  7510,\n",
    "         3181,  2998,  3220,   318,  5255,  1306,  6922,  7380,  1531,   429,\n",
    "         3221,   543,  6761,   476,  2500,  3245,  6571,  6199,  2387,  3096,\n",
    "         6814,  1354,   348,  6162,  4596,  1440,  2036,    37,  2460,  4590,\n",
    "         4686,  1363,  3447,   472,  6199,  3373,  7487,  6486,  3645,  1199,\n",
    "         5066,  4970,  2287,  3412,  1240,  4955,  3131,  1201,  6271,  7198,\n",
    "          269,  4951,  3373,  6953,  7351,  5871,  4321,  5711,   881,  6861,\n",
    "          486,  7192,  5103,  2289,  1198,  7394,  3459,  6045,  4550,  3345,\n",
    "          931,  3868,  5146,  4837,  4780,   703,  2965,  1157,  4676,  6203,\n",
    "         1610,  6941,   182,  7287,  4819,   226,  2869,  3686,  6024,  4145,\n",
    "         7489,  1223,  6486,  3614,   312,   550,  5259,  2174,  1198,  6723,\n",
    "          735,  2557,  5946,  1068,  6177,   974,  3898,  2788,  1610,  4409,\n",
    "         7834,   789,  7098,  6198,  8060,  6198,   295,   224,    67,  2561,\n",
    "         5737,  4407,  2384,   651,  7278,  4135,  2153,  5408,  3129,  7705,\n",
    "          476,  6248,  3033,  3706,  8196,  28862, 16848, 17016,\n",
    "         16704, 16672, 16647, 19521, 16414,  8710]])\n",
    "\n",
    "print(input_ids.shape)\n",
    "output = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n",
    "print(output[0])\n",
    "print(processor.tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Chameleon from transformers\n",
    "\n",
    "Load a chameleon model from transformers, and compare the outputs, the logits, and the hidden states to ensure we did a good job integrating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = hf_model.to(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Where is the capital of Germany?\"\n",
    "# image_path = \"/aifs4su/yaodong/datasets/aaa_dataset/T2I-preference/0812_t2i_preference_dataset/image/285ed1502f68ad9737af9b4c059d76b82984421692a22f08eff793a0cc6301e3/36ad46bb0f79b5eedaed2818b2744a19d5f89e52e410b43140a875daabebf088.png\"\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "# image = Image.open(image_path)\n",
    "input = processor(prompt, return_tensors=\"pt\").to(\n",
    "            hf_model.device, dtype=hf_model.dtype\n",
    "        )\n",
    "print(input.input_ids)\n",
    "input_ids = input.input_ids\n",
    "\n",
    "output = hf_model.generate(input_ids.to(hf_model.device), max_new_tokens=20, do_sample=False)\n",
    "print(processor.tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape of the weights\n",
    "# for name in hf_model.state_dict():\n",
    "#     print(name, hf_model.state_dict()[name].shape)\n",
    "    \n",
    "# print(hf_model.state_dict()[\"model.layers.0.self_attn.q_norm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.input_layernorm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.post_attention_layernorm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.self_attn.q_norm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.self_attn.q_proj.weight\"])\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_blocks_and_idxs = list(zip(range(hf_model.config.num_hidden_layers), hf_model.named_modules()))\n",
    "for i, block in hf_blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare logits with HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Where is the capital of Germany?\",\n",
    "    \"Calculate 2 * 42 = \", \n",
    "    \"My favorite\", \n",
    "    \"My favorite place is\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "hf_model.eval()\n",
    "tokenizer = processor.tokenizer\n",
    "prompt_ids = [tokenizer.encode(prompt, return_tensors=\"pt\") for prompt in prompts]\n",
    "tl_logits = [model(prompt_ids).detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "# hf logits are really slow as it's on CPU. If you have a big/multi-GPU machine, run `hf_model = hf_model.to(\"cuda\")` to speed this up\n",
    "logits = [hf_model(prompt_ids.to(hf_model.device)).logits.detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    if not torch.allclose(logits[i], tl_logits[i], atol=1e-2, rtol=1e-2):\n",
    "        print(f\"Logits for prompt {i} are not close\")\n",
    "        print(f\"Logits from HuggingFace: shape {logits[i].shape}\")\n",
    "        print(f\"Logits from TransformerLens: shape {tl_logits[i].shape}\")\n",
    "        diff = torch.abs(logits[i] - tl_logits[i]) > 1e-2\n",
    "        indices = torch.nonzero(diff)\n",
    "        for index in indices:\n",
    "            row, col, loc = index[0], index[1], index[2]\n",
    "            print(f\"Diff at {index}: HuggingFace={logits[i][row, col, loc]}, TransformerLens={tl_logits[i][row, col, loc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare hidden states\n",
    "\n",
    "tl_hidden_states = [model(prompt_ids, return_type=\"hidden_states\", stop_at_layer=1).detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "hf_hidden_states = [hf_model(prompt_ids.to(hf_model.device), output_hidden_states=True, output_attentions=True).hidden_states[1].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    print(f\"Shape of hf hidden states: {hf_hidden_states[i].shape}\")\n",
    "    print(f\"Shape of tl hidden states: {tl_hidden_states[i].shape}\")\n",
    "    if not torch.allclose(hf_hidden_states[i], tl_hidden_states[i], atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Hidden states for prompt {i} are not close\")\n",
    "    print(f\"Hidden states from HuggingFace: {hf_hidden_states[i]}\")\n",
    "    print(f\"Hidden states from TransformerLens: {tl_hidden_states[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare attentions\n",
    "\n",
    "tl_attentions = [model(prompt_ids, return_type=\"attentions\")[2].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "hf_attentions = [hf_model(prompt_ids.to(hf_model.device), output_hidden_states=True, output_attentions=True).attentions[2].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    print(f\"Shape of hf attentions: {hf_attentions[i].shape}\")\n",
    "    print(f\"Shape of tl attentions: {tl_attentions[i].shape}\")\n",
    "    if not torch.allclose(hf_attentions[i], tl_attentions[i], atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Attentions for prompt {i} are not close\")\n",
    "        print(f\"Attentions from HuggingFace: {hf_attentions[i]}\")\n",
    "        print(f\"Attentions from TransformerLens: {tl_attentions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerLens Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "llama_tokens = model.to_tokens(llama_text)\n",
    "llama_logits, llama_cache = model.run_with_cache(llama_tokens, remove_batch_dim=True)\n",
    "\n",
    "attention_pattern = llama_cache[\"pattern\", 0, \"attn\"]\n",
    "llama_str_tokens = model.to_str_tokens(llama_text)\n",
    "\n",
    "print(\"Layer 0 Head Attention Patterns:\")\n",
    "display(cv.attention.attention_patterns(tokens=llama_str_tokens, attention=attention_pattern))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_ablate = 0\n",
    "head_index_to_ablate = 31\n",
    "\n",
    "# We define a head ablation hook\n",
    "# The type annotations are NOT necessary, they're just a useful guide to the reader\n",
    "# \n",
    "def head_ablation_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, head_index_to_ablate, :] = 0.\n",
    "    return value\n",
    "\n",
    "original_loss = model(llama_tokens, return_type=\"loss\")\n",
    "ablated_loss = model.run_with_hooks(\n",
    "    llama_tokens, \n",
    "    return_type=\"loss\", \n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"v\", layer_to_ablate), \n",
    "        head_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
    "print(f\"Ablated Loss: {ablated_loss.item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('tl-llama': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f03ec946e3b5caa7cc710a963f479e62a68fff56c790a7066e03c8b5c22adad9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
